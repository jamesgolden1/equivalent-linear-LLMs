{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMfeBFeXkl46vOa5050nuDm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesgolden1/llms-are-llms/blob/main/notebooks/llama_3_3_70B_q4_locally_linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"huggingface_hub[hf_transfer]\""
      ],
      "metadata": {
        "id": "yfOoWhFL_mOW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mRz9ZdpYeb3J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"]='1'\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN_LLAMA_3_3')\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GITHUB_TOKEN']=userdata.get('GITHUB_TOKEN')"
      ],
      "metadata": {
        "id": "3jsio4cYqU2b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "6xwtn2tz9hAk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://jamesgolden1:$GITHUB_TOKEN@github.com/jamesgolden1/llms-are-llms.git"
      ],
      "metadata": {
        "id": "BxwNRTeTq8aR",
        "outputId": "e7a25906-00da-4c8a-bf6d-ed8ec9173f4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'llms-are-llms' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/llms-are-llms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q2ii5zer3_7",
        "outputId": "8f135ccf-3309-4e69-a4c2-944043c64c82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llms-are-llms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Model definition with detach() on nonlinearities at inference\n",
        "%%writefile /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\n",
        "# coding=utf-8\n",
        "# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n",
        "# and OPT implementations in this library. It has been modified from its\n",
        "# original forms to accommodate minor architectural differences compared\n",
        "# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "from functools import partial\n",
        "from typing import Callable, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "\n",
        "from ...activations import ACT2FN\n",
        "from ...cache_utils import Cache, DynamicCache, StaticCache\n",
        "from ...generation import GenerationMixin\n",
        "from ...modeling_attn_mask_utils import AttentionMaskConverter\n",
        "from ...modeling_flash_attention_utils import FlashAttentionKwargs\n",
        "from ...modeling_outputs import (\n",
        "    BaseModelOutputWithPast,\n",
        "    CausalLMOutputWithPast,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutputWithPast,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
        "from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
        "from ...processing_utils import Unpack\n",
        "from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n",
        "from ...utils import (\n",
        "    LossKwargs,\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    can_return_tuple,\n",
        "    is_torch_flex_attn_available,\n",
        "    logging,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from ...utils.deprecation import deprecate_kwarg\n",
        "from .configuration_llama import LlamaConfig\n",
        "\n",
        "\n",
        "if is_torch_flex_attn_available():\n",
        "    from torch.nn.attention.flex_attention import BlockMask\n",
        "\n",
        "    from ...integrations.flex_attention import make_flex_block_causal_mask\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"meta-llama/Llama-2-7b-hf\"\n",
        "_CONFIG_FOR_DOC = \"LlamaConfig\"\n",
        "\n",
        "\n",
        "class LlamaRMSNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        \"\"\"\n",
        "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "        if self.training:\n",
        "            hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "        else:\n",
        "\n",
        "            hidden_states = hidden_states * (torch.rsqrt(variance + self.variance_epsilon)).clone().detach()\n",
        "        return self.weight * hidden_states.to(input_dtype)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n",
        "\n",
        "\n",
        "ALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n",
        "\n",
        "\n",
        "class LlamaRotaryEmbedding(nn.Module):\n",
        "    def __init__(self, config: LlamaConfig, device=None):\n",
        "        super().__init__()\n",
        "        # BC: \"rope_type\" was originally \"type\"\n",
        "        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n",
        "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
        "        else:\n",
        "            self.rope_type = \"default\"\n",
        "        self.max_seq_len_cached = config.max_position_embeddings\n",
        "        self.original_max_seq_len = config.max_position_embeddings\n",
        "\n",
        "        self.config = config\n",
        "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
        "\n",
        "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self.original_inv_freq = self.inv_freq\n",
        "\n",
        "    @torch.no_grad()\n",
        "    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n",
        "    def forward(self, x, position_ids):\n",
        "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
        "        position_ids_expanded = position_ids[:, None, :].float()\n",
        "\n",
        "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
        "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
        "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1)\n",
        "            cos = emb.cos() * self.attention_scaling\n",
        "            sin = emb.sin() * self.attention_scaling\n",
        "\n",
        "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
        "\n",
        "\n",
        "def rotate_half(x):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
        "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
        "\n",
        "    Args:\n",
        "        q (`torch.Tensor`): The query tensor.\n",
        "        k (`torch.Tensor`): The key tensor.\n",
        "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
        "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
        "        position_ids (`torch.Tensor`, *optional*):\n",
        "            Deprecated and unused.\n",
        "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
        "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
        "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
        "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
        "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
        "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
        "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
        "    Returns:\n",
        "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
        "    \"\"\"\n",
        "    cos = cos.unsqueeze(unsqueeze_dim)\n",
        "    sin = sin.unsqueeze(unsqueeze_dim)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "\n",
        "class LlamaMLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
        "        self.act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "        else:\n",
        "\n",
        "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)).clone().detach() * self.up_proj(x))\n",
        "        return down_proj\n",
        "\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
        "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
        "    \"\"\"\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "\n",
        "def eager_attention_forward(\n",
        "    module: nn.Module,\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    scaling: float,\n",
        "    dropout: float = 0.0,\n",
        "    **kwargs,\n",
        "):\n",
        "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
        "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
        "\n",
        "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
        "    if attention_mask is not None:\n",
        "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
        "        attn_weights = attn_weights + causal_mask\n",
        "\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
        "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
        "    attn_output = torch.matmul(attn_weights, value_states)\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "\n",
        "    return attn_output, attn_weights\n",
        "\n",
        "\n",
        "class LlamaAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
        "        self.scaling = self.head_dim**-0.5\n",
        "        self.attention_dropout = config.attention_dropout\n",
        "        self.is_causal = True\n",
        "\n",
        "        self.q_proj = nn.Linear(\n",
        "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
        "        )\n",
        "        self.k_proj = nn.Linear(\n",
        "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
        "        )\n",
        "        self.v_proj = nn.Linear(\n",
        "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
        "        )\n",
        "        self.o_proj = nn.Linear(\n",
        "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
        "        attention_mask: Optional[torch.Tensor],\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        **kwargs: Unpack[FlashAttentionKwargs],\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        input_shape = hidden_states.shape[:-1]\n",
        "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
        "\n",
        "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "\n",
        "        cos, sin = position_embeddings\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        if not self.training:\n",
        "            query_states = query_states.clone().detach()\n",
        "            key_states = key_states.clone().detach()\n",
        "\n",
        "        if past_key_value is not None:\n",
        "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
        "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
        "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "        attention_interface: Callable = eager_attention_forward\n",
        "        if self.config._attn_implementation != \"eager\":\n",
        "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
        "                logger.warning_once(\n",
        "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
        "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
        "                )\n",
        "            else:\n",
        "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
        "        if self.training:\n",
        "            attn_output, attn_weights = attention_interface(\n",
        "                self,\n",
        "                query_states,\n",
        "                key_states,\n",
        "                value_states,\n",
        "                attention_mask,\n",
        "                dropout=0.0 if not self.training else self.attention_dropout,\n",
        "                scaling=self.scaling,\n",
        "                **kwargs,\n",
        "            )\n",
        "        else:\n",
        "            attn_output, attn_weights = attention_interface(\n",
        "                self,\n",
        "                query_states.clone().detach(),\n",
        "                key_states.clone().detach(),\n",
        "                value_states,\n",
        "                attention_mask,\n",
        "                dropout=0.0 if not self.training else self.attention_dropout,\n",
        "                scaling=self.scaling,\n",
        "                **kwargs,\n",
        "            )\n",
        "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "\n",
        "class LlamaDecoderLayer(nn.Module):\n",
        "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
        "\n",
        "        self.mlp = LlamaMLP(config)\n",
        "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
        "        **kwargs: Unpack[FlashAttentionKwargs],\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "        residual = hidden_states\n",
        "\n",
        "        hidden_states = self.input_layernorm(hidden_states)\n",
        "\n",
        "        # Self Attention\n",
        "        hidden_states, self_attn_weights = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_value=past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "            use_cache=use_cache,\n",
        "            cache_position=cache_position,\n",
        "            position_embeddings=position_embeddings,\n",
        "            **kwargs,\n",
        "        )\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "LLAMA_START_DOCSTRING = r\"\"\"\n",
        "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "    etc.)\n",
        "\n",
        "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "    and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config ([`LlamaConfig`]):\n",
        "            Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
        "            load the weights associated with the model, only the configuration. Check out the\n",
        "            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n",
        "    LLAMA_START_DOCSTRING,\n",
        ")\n",
        "class LlamaPreTrainedModel(PreTrainedModel):\n",
        "    config_class = LlamaConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = [\"LlamaDecoderLayer\"]\n",
        "    _skip_keys_device_placement = [\"past_key_values\"]\n",
        "    _supports_flash_attn_2 = True\n",
        "    _supports_sdpa = True\n",
        "    _supports_flex_attn = True\n",
        "    _supports_cache_class = True\n",
        "    _supports_quantized_cache = True\n",
        "    _supports_static_cache = True\n",
        "    _supports_attention_backend = True\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.initializer_range\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "\n",
        "LLAMA_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "            it.\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            [What are attention masks?](../glossary#attention-mask)\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n",
        "            `past_key_values`).\n",
        "\n",
        "            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n",
        "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
        "            information on the default strategy.\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
        "            config.n_positions - 1]`.\n",
        "\n",
        "            [What are position IDs?](../glossary#position-ids)\n",
        "        past_key_values (`Cache`, *optional*):\n",
        "            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n",
        "            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n",
        "\n",
        "            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n",
        "\n",
        "            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n",
        "            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n",
        "            of shape `(batch_size, sequence_length)`.\n",
        "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
        "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
        "            model's internal embedding lookup matrix.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
        "            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n",
        "            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n",
        "            the complete sequence length.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n",
        "    LLAMA_START_DOCSTRING,\n",
        ")\n",
        "class LlamaModel(LlamaPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
        "\n",
        "    Args:\n",
        "        config: LlamaConfig\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig):\n",
        "        super().__init__(config)\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
        "        )\n",
        "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    @can_return_tuple\n",
        "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n",
        "    ) -> BaseModelOutputWithPast:\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "\n",
        "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
        "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
        "\n",
        "        if self.gradient_checkpointing and self.training and use_cache:\n",
        "            logger.warning_once(\n",
        "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
        "            )\n",
        "            use_cache = False\n",
        "\n",
        "        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n",
        "        if not isinstance(past_key_values, (type(None), Cache)):\n",
        "            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "        if use_cache and past_key_values is None:\n",
        "            past_key_values = DynamicCache()\n",
        "\n",
        "        if cache_position is None:\n",
        "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
        "            cache_position = torch.arange(\n",
        "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
        "            )\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = cache_position.unsqueeze(0)\n",
        "\n",
        "        causal_mask = self._update_causal_mask(\n",
        "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
        "        )\n",
        "\n",
        "        hidden_states = inputs_embeds\n",
        "\n",
        "        # create position embeddings to be shared across the decoder layers\n",
        "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "\n",
        "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                layer_outputs = self._gradient_checkpointing_func(\n",
        "                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n",
        "                    hidden_states,\n",
        "                    causal_mask,\n",
        "                    position_ids,\n",
        "                    past_key_values,\n",
        "                    output_attentions,\n",
        "                    use_cache,\n",
        "                    cache_position,\n",
        "                    position_embeddings,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=causal_mask,\n",
        "                    position_ids=position_ids,\n",
        "                    past_key_value=past_key_values,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                    cache_position=cache_position,\n",
        "                    position_embeddings=position_embeddings,\n",
        "                    **flash_attn_kwargs,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        return BaseModelOutputWithPast(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=past_key_values if use_cache else None,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "        )\n",
        "\n",
        "    def _update_causal_mask(\n",
        "        self,\n",
        "        attention_mask: torch.Tensor,\n",
        "        input_tensor: torch.Tensor,\n",
        "        cache_position: torch.Tensor,\n",
        "        past_key_values: Cache,\n",
        "        output_attentions: bool = False,\n",
        "    ):\n",
        "        if self.config._attn_implementation == \"flash_attention_2\":\n",
        "            if attention_mask is not None and (attention_mask == 0.0).any():\n",
        "                return attention_mask\n",
        "            return None\n",
        "        if self.config._attn_implementation == \"flex_attention\":\n",
        "            if isinstance(attention_mask, torch.Tensor):\n",
        "                attention_mask = make_flex_block_causal_mask(attention_mask)\n",
        "            if isinstance(attention_mask, BlockMask):\n",
        "                return attention_mask\n",
        "\n",
        "        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n",
        "        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n",
        "        # to infer the attention mask.\n",
        "        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
        "        using_static_cache = isinstance(past_key_values, StaticCache)\n",
        "\n",
        "        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n",
        "        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n",
        "            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n",
        "                attention_mask,\n",
        "                inputs_embeds=input_tensor,\n",
        "                past_key_values_length=past_seen_tokens,\n",
        "                is_training=self.training,\n",
        "            ):\n",
        "                return None\n",
        "\n",
        "        dtype, device = input_tensor.dtype, input_tensor.device\n",
        "        sequence_length = input_tensor.shape[1]\n",
        "        if using_static_cache:\n",
        "            target_length = past_key_values.get_max_cache_shape()\n",
        "        else:\n",
        "            target_length = (\n",
        "                attention_mask.shape[-1]\n",
        "                if isinstance(attention_mask, torch.Tensor)\n",
        "                else past_seen_tokens + sequence_length + 1\n",
        "            )\n",
        "\n",
        "        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n",
        "        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
        "            attention_mask,\n",
        "            sequence_length=sequence_length,\n",
        "            target_length=target_length,\n",
        "            dtype=dtype,\n",
        "            device=device,\n",
        "            cache_position=cache_position,\n",
        "            batch_size=input_tensor.shape[0],\n",
        "        )\n",
        "\n",
        "        if (\n",
        "            self.config._attn_implementation == \"sdpa\"\n",
        "            and attention_mask is not None\n",
        "            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n",
        "            and not output_attentions\n",
        "        ):\n",
        "            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n",
        "            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n",
        "            # Details: https://github.com/pytorch/pytorch/issues/110213\n",
        "            min_dtype = torch.finfo(dtype).min\n",
        "            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n",
        "\n",
        "        return causal_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_4d_causal_attention_mask_with_cache_position(\n",
        "        attention_mask: torch.Tensor,\n",
        "        sequence_length: int,\n",
        "        target_length: int,\n",
        "        dtype: torch.dtype,\n",
        "        device: torch.device,\n",
        "        cache_position: torch.Tensor,\n",
        "        batch_size: int,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n",
        "        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n",
        "\n",
        "        Args:\n",
        "            attention_mask (`torch.Tensor`):\n",
        "                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n",
        "                `(batch_size, 1, query_length, key_value_length)`.\n",
        "            sequence_length (`int`):\n",
        "                The sequence length being processed.\n",
        "            target_length (`int`):\n",
        "                The target length: when generating with static cache, the mask should be as long as the static cache,\n",
        "                to account for the 0 padding, the part of the cache that is not filled yet.\n",
        "            dtype (`torch.dtype`):\n",
        "                The dtype to use for the 4D attention mask.\n",
        "            device (`torch.device`):\n",
        "                The device to place the 4D attention mask on.\n",
        "            cache_position (`torch.Tensor`):\n",
        "                Indices depicting the position of the input sequence tokens in the sequence.\n",
        "            batch_size (`torch.Tensor`):\n",
        "                Batch size.\n",
        "        \"\"\"\n",
        "        if attention_mask is not None and attention_mask.dim() == 4:\n",
        "            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n",
        "            causal_mask = attention_mask\n",
        "        else:\n",
        "            min_dtype = torch.finfo(dtype).min\n",
        "            causal_mask = torch.full(\n",
        "                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n",
        "            )\n",
        "            if sequence_length != 1:\n",
        "                causal_mask = torch.triu(causal_mask, diagonal=1)\n",
        "            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
        "            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
        "            if attention_mask is not None:\n",
        "                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
        "                mask_length = attention_mask.shape[-1]\n",
        "                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n",
        "                    causal_mask.device\n",
        "                )\n",
        "                padding_mask = padding_mask == 0\n",
        "                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
        "                    padding_mask, min_dtype\n",
        "                )\n",
        "\n",
        "        return causal_mask\n",
        "\n",
        "\n",
        "class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n",
        "\n",
        "\n",
        "class LlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
        "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = LlamaModel(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model\n",
        "\n",
        "    @can_return_tuple\n",
        "    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n",
        "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
        "        **kwargs: Unpack[KwargsForCausalLM],\n",
        "    ) -> CausalLMOutputWithPast:\n",
        "        r\"\"\"\n",
        "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
        "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
        "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
        "\n",
        "            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n",
        "                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n",
        "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
        "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
        "                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n",
        "                This is useful when using packed tensor format (single dimension for batch and sequence length).\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```python\n",
        "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "\n",
        "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
        "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        >>> # Generate\n",
        "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
        "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
        "        ```\"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        outputs: BaseModelOutputWithPast = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            cache_position=cache_position,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
        "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
        "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n",
        "\n",
        "    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n",
        "    (e.g. GPT-2) do.\n",
        "\n",
        "    Since it does classification on the last token, it requires to know the position of the last token. If a\n",
        "    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n",
        "    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n",
        "    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n",
        "    each row of the batch).\n",
        "    \"\"\",\n",
        "    LLAMA_START_DOCSTRING,\n",
        ")\n",
        "class LlamaForSequenceClassification(LlamaPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.model = LlamaModel(config)\n",
        "        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    @can_return_tuple\n",
        "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "    ) -> SequenceClassifierOutputWithPast:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "\n",
        "        transformer_outputs: BaseModelOutputWithPast = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        hidden_states = transformer_outputs.last_hidden_state\n",
        "        logits = self.score(hidden_states)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            batch_size = input_ids.shape[0]\n",
        "        else:\n",
        "            batch_size = inputs_embeds.shape[0]\n",
        "\n",
        "        if self.config.pad_token_id is None and batch_size != 1:\n",
        "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
        "        if self.config.pad_token_id is None:\n",
        "            last_non_pad_token = -1\n",
        "        elif input_ids is not None:\n",
        "            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n",
        "            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n",
        "            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n",
        "            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n",
        "        else:\n",
        "            last_non_pad_token = -1\n",
        "            logger.warning_once(\n",
        "                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
        "                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
        "            )\n",
        "\n",
        "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=pooled_logits,\n",
        "            past_key_values=transformer_outputs.past_key_values,\n",
        "            hidden_states=transformer_outputs.hidden_states,\n",
        "            attentions=transformer_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "The Llama Model transformer with a span classification head on top for extractive question-answering tasks like\n",
        "SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    LLAMA_START_DOCSTRING,\n",
        ")\n",
        "class LlamaForQuestionAnswering(LlamaPreTrainedModel):\n",
        "    base_model_prefix = \"transformer\"\n",
        "\n",
        "    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Llama\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.transformer = LlamaModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.transformer.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.transformer.embed_tokens = value\n",
        "\n",
        "    @can_return_tuple\n",
        "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        start_positions: Optional[torch.LongTensor] = None,\n",
        "        end_positions: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        **kwargs,\n",
        "    ) -> QuestionAnsweringModelOutput:\n",
        "        r\"\"\"\n",
        "        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
        "            are not taken into account for computing the loss.\n",
        "        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
        "            are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "\n",
        "        outputs: BaseModelOutputWithPast = self.transformer(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    The Llama Model transformer with a token classification head on top (a linear layer on top of the hidden-states\n",
        "    output) e.g. for Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    LLAMA_START_DOCSTRING,\n",
        ")\n",
        "class LlamaForTokenClassification(LlamaPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.model = LlamaModel(config)\n",
        "        if getattr(config, \"classifier_dropout\", None) is not None:\n",
        "            classifier_dropout = config.classifier_dropout\n",
        "        elif getattr(config, \"hidden_dropout\", None) is not None:\n",
        "            classifier_dropout = config.hidden_dropout\n",
        "        else:\n",
        "            classifier_dropout = 0.1\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.score = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    @can_return_tuple\n",
        "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "    ) -> TokenClassifierOutput:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "\n",
        "        outputs: BaseModelOutputWithPast = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.score(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_function(logits, labels, self.config)\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"LlamaForCausalLM\",\n",
        "    \"LlamaModel\",\n",
        "    \"LlamaPreTrainedModel\",\n",
        "    \"LlamaForSequenceClassification\",\n",
        "    \"LlamaForQuestionAnswering\",\n",
        "    \"LlamaForTokenClassification\",\n",
        "]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "atWpGR4UGRAu",
        "outputId": "72daa35e-1117-4c9d-9d12-2b7ef642bf3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.JacobianAnalyzer import *"
      ],
      "metadata": {
        "id": "nS_9_lzUvyJl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.JacobianAnalyzer import JacobianAnalyzer as JacobianAnalyzer"
      ],
      "metadata": {
        "id": "s3ZIImFTsnok"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from models.llama_3.llama_3_forward import model_forward\n",
        "# setattr(JacobianAnalyzer, 'model_forward', model_forward)"
      ],
      "metadata": {
        "id": "mAT2pAC1s12f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Jacobian Analysis\n",
        "%%time\n",
        "# %%writefile run_llama.py\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import argparse\n",
        "\n",
        "run_all=False\n",
        "\n",
        "# Initialize the analyzer\n",
        "analyzer = JacobianAnalyzer(model_name=\"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\")\n",
        "\n",
        "text = 'The Golden'\n",
        "max_new_tokens=1\n",
        "temperature=1e-6\n",
        "\n",
        "# Generate output\n",
        "analyzer.generate(text, max_new_tokens, temperature);\n",
        "\n",
        "analyzer.model.lm_head.to('cpu')\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# # # Compute Jacobian\n",
        "analyzer.compute_jacobian()\n",
        "# analyzer.compute_jacobian_nonlinear()\n",
        "# analyzer.plot_jacobian_comparison(text,filename_png=\"fig3\")\n",
        "\n",
        "# # analyzer.compute_jacobian_row_col_norm(n_components=8)#, svs=1)\n",
        "# # analyzer.plot_singular_values(mode=\"row_col_vectors\",filename_png=\"fig4_col\")\n",
        "\n",
        "# analyzer.compute_jacobian_svd(n_components=24, svs=1)\n",
        "# analyzer.plot_singular_values(title=\"SVD\",filename_png=\"fig4_svd\")\n",
        "\n",
        "# analyzer.plot_jacobian_image(filename_png=\"fig2\")\n",
        "\n",
        "if run_all:\n",
        "    layerlist=list(range(1,32))\n",
        "    # layerlist.extend([26,27])\n",
        "    analyzer.compute_jacobian_layers_svd(layerlist=layerlist,n_components=24,svs=8)#,filename=\"fig5_svd_layers_llama_3_2\")\n",
        "    analyzer.plot_singular_values(title=\"SVD Layers\",mode='singular_vectors_layers',key='layer',filename_png=\"fig5_svd_layers\")\n",
        "\n",
        "    analyzer.compute_jacobian_layers_svd(layerlist=layerlist,n_components=4,svs=2,key='mlp')#,filename=\"fig5_svd_layers_llama_3_2_mlp\")\n",
        "    analyzer.plot_singular_values(title=\"SVD MLP\",mode='singular_vectors_layers',key='mlp',filename_png=\"fig5_svd_mlp\")\n",
        "\n",
        "    analyzer.compute_jacobian_layers_svd(layerlist=layerlist,n_components=4,svs=2,key='attn')#,filename=\"fig5_svd_layers_llama_3_2_attn\")\n",
        "    analyzer.plot_singular_values(title=\"SVD Attn\",mode='singular_vectors_layers',key='attn',filename_png=\"fig5_svd_attn\")\n",
        "\n",
        "    analyzer.plot_path(filename_png=\"fig6_path\")\n",
        "    analyzer.plot_dimensionality(filename_png=\"fig6_dimensionality\")\n",
        "\n",
        "    analyzer.compute_jacobian_layers_svd(layerlist=layerlist,layer_mode='layerwise',n_components=4,svs=2)#,filename=\"fig5_svd_layers_llama_3_2\")\n",
        "    analyzer.plot_singular_values(title=\"SVD Layers\",mode='singular_vectors_layers_layerwise',key='layer',filename_png=\"fig5_svd_layers_layerwise\")\n",
        "\n",
        "    analyzer.compute_jacobian_layers_svd(layerlist=layerlist,layer_mode='layerwise',n_components=4,svs=2,key='mlp')#,filename=\"fig5_svd_layers_llama_3_2_mlp\")\n",
        "    analyzer.plot_singular_values(title=\"SVD MLP\",mode='singular_vectors_layers_layerwise',key='mlp',filename_png=\"fig5_svd_mlp_layerwise\")\n",
        "\n",
        "    analyzer.compute_jacobian_layers_svd(layerlist=layerlist,layer_mode='layerwise',n_components=4,svs=2,key='attn')#,filename=\"fig5_svd_layers_llama_3_2_attn\")\n",
        "    analyzer.plot_singular_values(title=\"SVD Attn\",mode='singular_vectors_layers_layerwise',key='attn',filename_png=\"fig5_svd_attn_layerwise\")\n",
        "\n",
        "    analyzer.plot_path(filename_png=\"fig6_dimensionality_layerwise\")\n",
        "\n",
        "# # if __name__ == \"__main__\":\n",
        "# #     main()"
      ],
      "metadata": {
        "id": "uZbxv6Ns17uw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "4714af69902f443494ee3f2d7e454789",
            "930242be9a5947879d8fccabcf8d066f",
            "482e1c70dc604308b7d4e6d88d4dc976",
            "df69d8bdeb904d448f20abe920a7a31f",
            "6862aff19194478ab8545922024a0cd1",
            "1a4b21b9f2594b628dcaf327722b9df9",
            "b1e27689fcc849e6a8734f81b28253a3",
            "69fb36ee468043cd922c968f1e23eee1",
            "55efb9355b1041fa926b2d0295cc0b4e",
            "29fe2f209db04fcdbe1343bf7c235cb0",
            "f9d819558ea64d86a3c8862142e9f0e0"
          ]
        },
        "outputId": "b77d821b-4098-4f7c-d94e-37db59b199ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4714af69902f443494ee3f2d7e454789"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7min 9s, sys: 2min 53s, total: 10min 3s\n",
            "Wall time: 10min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "UutPYZtz-H_H",
        "outputId": "4a5437bf-f93c-4926-c8ff-9927b55977c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May  9 17:43:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0            131W /  400W |   39849MiB /  40960MiB |     61%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm model_forward exactly matches outputs from generate (note: this is not the jacobian reconstruction)\n",
        "model_forward_error = analyzer.embeds_predicted-analyzer.model_forward(analyzer.embeds)\n",
        "print(\"model_forward_error:\", model_forward_error)\n",
        "print(\"model_forward_error allclose:\",torch.allclose(analyzer.embeds_predicted,analyzer.model_forward(analyzer.embeds)))"
      ],
      "metadata": {
        "id": "5YijCNIyC9Rp",
        "outputId": "a0a2574c-232d-48a9-ee06-315320040ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_forward_error: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SubBackward0>)\n",
            "model_forward_error allclose: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "pr=pearsonr(analyzer.embeds_predicted.cpu().detach().float().numpy(), analyzer.linear_jacobian_output.cpu().detach().float().numpy()),\n",
        "sr,spsig=spearmanr(analyzer.embeds_predicted.cpu().detach().float().numpy(), analyzer.linear_jacobian_output.cpu().detach().float().numpy())\n",
        "# pearsonr[0], spearmanr"
      ],
      "metadata": {
        "id": "ZCEPCHhfK0q6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearsonr(analyzer.embeds_predicted.cpu().detach().float().numpy(), analyzer.linear_jacobian_output.cpu().detach().float().numpy()), spearmanr(analyzer.embeds_predicted.cpu().detach().float().numpy(), analyzer.linear_jacobian_output.cpu().detach().float().numpy())"
      ],
      "metadata": {
        "id": "5_qTqC4NK38m",
        "outputId": "8a7d794d-84a3-408c-c704-c47bd051e724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PearsonRResult(statistic=np.float32(0.9999699), pvalue=np.float64(0.0)),\n",
              " SignificanceResult(statistic=np.float64(0.9999511117058009), pvalue=np.float64(0.0)))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jacobian_output_error = analyzer.linear_jacobian_output - analyzer.model_forward(analyzer.embeds)#outputs.hidden_states[-1][-1][0, -1]\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
        "axs = axs.flat\n",
        "for n, ax in enumerate(axs):\n",
        "\n",
        "    if n == 0:\n",
        "        # ax.scatter(outputs.hidden_states[-1][-1][-1][-1].cpu().detach().float().numpy(),\\\n",
        "        ax.scatter(analyzer.model_forward(analyzer.embeds).cpu().detach().float().numpy(),\\\n",
        "                    (analyzer.linear_jacobian_output).cpu().detach().float().numpy(),.2,marker='x');\n",
        "        ax.set_xlabel('Predicted Output Embedding')\n",
        "        ax.set_ylabel('Jacobian * (input embedding matrices)')\n",
        "        ax.set_title('Llama 3.3 70B IT BNB Q4\\nOutput vs. (Jacobian*Input)')\n",
        "        ax.grid()\n",
        "    if n == 1:\n",
        "        outnp = jacobian_output_error.cpu().detach().float().numpy()\n",
        "        ax.hist(outnp,bins=40)#,range=[-20,20])#,range=[-7.5e-6,7.5e-6]);\n",
        "        ax.set_ylabel('Counts')\n",
        "        ax.set_xlabel('Output - Jacobian * (input embedding matrices)')\n",
        "        ax.grid()\n",
        "        ax.set_title('Llama 3.3 70B IT BNB Q4\\nHistogram of Output - (Jacobian*Input)\\nstdev(err) / stdev(output) = '+\"{val:1.2e}\".format(val=np.std(outnp)/np.std(analyzer.embeds_predicted.cpu().detach().float().numpy())))"
      ],
      "metadata": {
        "id": "sGRnqTWtBDMo",
        "outputId": "135cae8d-a8c5-40a0-f6f4-6544b3c2a560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
